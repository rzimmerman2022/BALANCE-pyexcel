feat: Add explicit schemas for Jordyn's bank statements

Macro-Level Summary:
This commit introduces explicit YAML-based schemas for processing three of Jordyn's financial data sources: Wells Fargo Visa, Chase Checking, and Discover Card. This work extends the systematic, evidence-based schema creation methodology previously applied to Ryan's data, ensuring robust and predictable ETL processing for a wider range of CSV formats. The `schema_registry.yml` has been updated to incorporate these new schemas, enabling the system to correctly route each data file to its specific processor.

Micro-Level Granular Details & AI Coding Insights:

1.  **New Schema Files Created:**
    *   `rules/jordyn_wellsfargo_visa_v1.yaml`
    *   `rules/jordyn_chase_checking_v1.yaml`
    *   `rules/jordyn_discover_card_v1.yaml`

2.  **Methodology Applied (Iterative Refinement for each schema):**
    *   **Evidence Gathering:** Initial diagnostic runs (`poetry run python src/balance_pipeline/cli.py -v <file_path> dummy_workbook.xlsx --dry-run`) were performed on each sample CSV. This step was crucial for observing the raw CSV headers (`[SCHEMA_MATCH_INPUT]` log) and how the system initially attempted to process the file (typically falling back to `generic_csv`).
    *   **Schema Definition (Initial Draft):** Based on the diagnostic evidence, an initial version of each schema was drafted. This included defining:
        *   `id`: A unique identifier for the schema.
        *   `description`: A human-readable description.
        *   `version`: Schema version.
        *   `header_signature`: The exact list of column headers observed in the CSV. This is critical for the schema matching logic.
        *   `column_map`: Mapping of CSV header names to the internal master schema column names (e.g., "Transaction Date" to "Date").
        *   `derived_columns`: Rules for creating new columns based on static values (e.g., `Owner: Jordyn`) or transformations of existing columns (e.g., extracting `AccountLast4` via regex).
        *   `transformations`: Containing sub-sections like:
            *   `extras_ignore`: To specify CSV columns that should not be carried over into the `Extras` field.
            *   `date_formats`: To provide specific parsing patterns for date columns.
            *   `amount_to_numeric`: Boolean, typically true.
            *   `amount_sign_rule`: To standardize positive/negative conventions for financial amounts (e.g., `as_is`, `invert_sign`).
    *   **Iterative Verification & Refinement Loop:** This was the core of the evidence-based approach for each schema:
        1.  The new schema `.yaml` file was created.
        2.  The schema was registered in `rules/schema_registry.yml` by adding an entry like `{ id: new_schema_id, file: new_schema_file.yaml }`.
        3.  The diagnostic command was re-run against the specific CSV.
        4.  Diagnostic logs (`[SCHEMA_RESULT]`, `[APPLY_SCHEMA_WARN]`, `[APPLY_SCHEMA_INTEGRITY]`, `[APPLY_SCHEMA_STATE]`) were meticulously analyzed for:
            *   Correct schema selection.
            *   Warnings related to column mapping, derived column rules, or transformations.
            *   Integrity issues like missing master columns or unexpected surviving columns.
        5.  The schema definition in its `.yaml` file was refined based on these logs. For example:
            *   Correcting the key for static derived column rules from `rule_type: static_value` to `rule: static_value` after observing `Unknown rule type` warnings. This was guided by inspecting `generic_csv.yaml`.
            *   Adjusting `amount_sign_rule` after inspecting the `dummy_workbook.dry-run.csv` output to match master schema conventions (debits negative, credits positive).
            *   Specifying exact `date_formats` (e.g., `"%Y-%m-%d"` or `"%m/%d/%Y"`) based on observed data.
            *   Refining `AccountLast4` regex patterns and source columns.
        6.  The diagnostic command was run again (Loop back to step 3). This cycle continued until the schema processed the data cleanly with minimal or expected warnings.

3.  **Specific Schema Details & Rationale:**

    *   **`rules/jordyn_wellsfargo_visa_v1.yaml`:**
        *   Target File: `sample_data_multi/Jordyn/Jordyn - Wells Fargo - Active Cash Visa Signature Card x4296 - CSV.csv`
        *   `header_signature`: Includes "Name", "Institution", "Account Description", "Statement Period Description", "Statement Start Date", "Statement End Date", "Date", "Post Date", "Description", "Reference Number", "Amount", "Category".
        *   `column_map`: Maps "Account Description" to `Account`, "Statement Period Description" to `StatementPeriodDesc`, etc.
        *   `derived_columns`:
            *   `Owner: { rule: static_value, value: Jordyn }`
            *   `DataSourceName: { rule: static_value, value: WellsFargoVisa }`
            *   `AccountType: { rule: static_value, value: Credit Card }` (Master schema field)
            *   `AccountLast4: { rule: regex_extract, column: Account, pattern: "(?:x|\\.{3}|\\()(?P<last4>\\d{4})\\)?$" }`
        *   `transformations`:
            *   `extras_ignore`: Explicitly lists `Name`, `Institution`.
            *   `date_formats`: `Date: "%m/%d/%Y"`, `PostDate: "%m/%d/%Y"`, etc.
            *   `amount_sign_rule: "as_is"` (CSV purchases are positive, payments negative, matching desired intermediate state before master schema sign convention if it differs).

    *   **`rules/jordyn_chase_checking_v1.yaml`:**
        *   Target File: `sample_data_multi/Jordyn/Jordyn - Chase Bank - Total Checking x6173 - All.csv`
        *   `header_signature`: Includes "Transaction Date" instead of "Date".
        *   `column_map`: Maps "Transaction Date" to `Date`, CSV "Account Type" (e.g., "Total Checking x6173") to master `Account`.
        *   `derived_columns`:
            *   `Owner: { rule: static_value, value: Jordyn }`
            *   `DataSourceName: { rule: static_value, value: ChaseChecking }`
            *   `AccountType: { rule: static_value, value: Checking }` (Master schema field)
            *   `AccountLast4: { rule: regex_extract, column: Account, pattern: "(?:x|\\.{3}|\\()(?P<last4>\\d{4})\\)?$" }`
        *   `transformations`:
            *   `extras_ignore`: `Name`, `Institution`.
            *   `date_formats`: `Date: "%Y-%m-%d"`, `PostDate: "%Y-%m-%d"`, etc. (Chase uses YYYY-MM-DD).
            *   `amount_sign_rule: "invert_sign"` (CSV withdrawals positive, deposits negative; master schema expects debits negative, credits positive).

    *   **`rules/jordyn_discover_card_v1.yaml`:**
        *   Target File: `sample_data_multi/Jordyn/Jordyn - Discover - Discover It Card x1544 - CSV.csv`
        *   `header_signature`: Includes "Card Type", "Statement Period".
        *   `column_map`: Maps "Card Type" (e.g., "Discover It Card x1544") to `Account`, "Statement Period" to `StatementPeriodDesc`.
        *   `derived_columns`:
            *   `Owner: { rule: static_value, value: Jordyn }`
            *   `DataSourceName: { rule: static_value, value: DiscoverCard }`
            *   `AccountType: { rule: static_value, value: Credit Card }` (Master schema field)
            *   `AccountLast4: { rule: regex_extract, column: Account, pattern: "(?:x|\\.{3}|\\()(?P<last4>\\d{4})\\)?$" }`
        *   `transformations`:
            *   `extras_ignore`: `Name`, `Institution`.
            *   `date_formats`: `Date: "%Y-%m-%d"`, `PostDate: "%Y-%m-%d"` (Discover uses YYYY-MM-DD).
            *   `amount_sign_rule: "invert_sign"` (CSV payments are negative; master schema expects credits positive).

4.  **Modification to `rules/schema_registry.yml`:**
    *   Appended entries for `jordyn_wellsfargo_visa_v1`, `jordyn_chase_checking_v1`, and `jordyn_discover_card_v1`, each pointing to their respective `.yaml` file via the `file:` key. This modular approach keeps the registry cleaner and schema definitions self-contained.

5.  **System Integration Testing:**
    *   A final diagnostic run (`poetry run python src/balance_pipeline/cli.py -v sample_data_multi/ dummy_workbook.xlsx --dry-run`) processed all 5 sample files (3 for Jordyn, 2 for Ryan).
    *   The `[PROCESS_SUMMARY] Schema matching counts` log confirmed that each file was correctly routed to its specific explicit schema (`jordyn_chase_checking_v1`: 1, `jordyn_discover_card_v1`: 1, `jordyn_wellsfargo_visa_v1`: 1, `ryan_monarch_v1`: 1, `ryan_rocket_v1`: 1), with no fallbacks to `generic_csv`. This validated the successful integration and correct functioning of the schema dispatch logic.

**AI Coding Implications & Learnings from this Session:**
*   **Schema Specificity is Paramount:** Generic solutions struggle with the heterogeneity of real-world CSVs. Explicit, evidence-based schemas are essential for reliability.
*   **Iterative Debugging Cycle:** The "define -> test -> analyze logs -> refine" cycle is highly effective for complex configurations. AI tools assisting in this should support or emulate this iterative feedback loop.
*   **Configuration Nuances:** Seemingly minor differences in configuration syntax (e.g., `rule` vs. `rule_type`) can significantly impact behavior. Access to working examples (`generic_csv.yaml`) was key to resolving such issues. An AI coding assistant would benefit from being trained on the specific DSLs and configuration patterns of the target system.
*   **Diagnostic Logging as Ground Truth:** Comprehensive and detailed logging (like the `[APPLY_SCHEMA_WARN]`, `[APPLY_SCHEMA_INTEGRITY]` messages) is invaluable for pinpointing issues. AI analysis of these logs could accelerate debugging.
*   **Handling of "Extra" Columns:** The behavior of `extras_ignore` for columns also present in `header_signature` (like the 'Name' column consistently appearing in "Unexpected columns survived mapping" logs despite being in `extras_ignore`) suggests areas where system behavior might have subtle complexities. While not critical for this task's success (as the core data was processed correctly), it's a point of learning for deeper system understanding.

This commit significantly enhances the system's capability to process diverse financial data accurately.
