Refactor: Implement evidence-based schema normalization for ETL pipeline

This commit overhauls the ETL pipeline's schema handling to be driven by
evidence-based analysis of source data, improving robustness and accuracy.

Key Changes:

1.  **Schema Discovery & Definition:**
    *   Introduced `audit_all_sources.py` to perform comprehensive analysis
        of source CSVs, reporting on column coverage, data types, and
        population statistics.
    *   Defined a new canonical schema in `rules/canonical_schema.yml` based
        on the audit findings. This schema includes metadata, required, and
        optional columns with clear rationale.
    *   Updated `MASTER_SCHEMA_COLUMNS` in `src/balance_pipeline/constants.py`
        to reflect the new 25-column canonical schema.

2.  **Individual Schema Refinements (`rules/*.yaml`):**
    *   Reviewed and updated all source-specific schemas:
        - `jordyn_chase_checking_v1.yaml`
        - `jordyn_discover_card_v1.yaml`
        - `jordyn_wells_v1.yaml`
        - `ryan_monarch_v1.yaml`
        - `ryan_rocket_v1.yaml`
        - `generic_csv.yaml`
    *   Adjustments include corrected `column_map` entries to align with the
        new canonical names, mapping of previously unmapped fields identified
        by the audit, standardization of derived column syntax, and updates
        to `extras_ignore` lists.
    *   Fixed `id` field and `file_pattern` quoting in `ryan_monarch_v2.yaml`
        and `ryan_rocket_v2.yaml` to ensure proper loading by the schema registry.

3.  **Pipeline Code Enhancements:**
    *   Modified `src/balance_pipeline/csv_consolidator.py` to correctly
        parse the `type` key for derived column rules in schema YAMLs,
        resolving previous processing errors.
    *   Updated `src/balance_pipeline/cli.py` to ensure the output directory
        is created before writing Parquet or Excel files, preventing file
        save errors.

4.  **Validation & Output:**
    *   The pipeline now successfully processes source CSVs using the updated
        schemas and generates `output/balance_final.parquet` conforming to
        the new canonical schema.
    *   Introduced `audit_parquet_output.py` to validate the schema of the
        generated Parquet file against `rules/canonical_schema.yml`. This
        confirms adherence to the defined metadata, required, and optional
        columns, and checks for unexpected columns.
    *   Added `check_parquet.py` as a utility to quickly inspect the
        generated Parquet file.

This refactoring establishes a more data-driven and maintainable approach
to schema management within the ETL pipeline.
