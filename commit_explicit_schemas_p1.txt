feat: Implement explicit YAML schemas and refactor schema loading (Phase 1)

This commit introduces a foundational shift towards explicit, evidence-based YAML schemas for CSV processing, enhancing pipeline robustness and maintainability. It includes the initial refactoring of the schema loading mechanism and the creation of the first set of new, self-contained schema definitions.

Accomplishments in this Phase:

1.  **Refactored Schema Loading & Matching (`src/balance_pipeline/schema_registry.py`):**
    *   Modified the schema registry to load full schema definitions directly from individual `.yaml` files located in the `rules/` directory.
    *   Updated the matching logic to primarily use the `header_signature` from these self-contained schema files for more precise CSV identification.
    *   This moves away from a split-logic system (separate matching rules and a monolithic transformation rule file) towards a more integrated and explicit model where each schema file is the single source of truth for a given CSV format.

2.  **Created New Explicit Schemas:**
    *   `rules/ryan_monarch_v1.yaml`: New schema for Ryan's Monarch Money exports.
    *   `rules/ryan_rocket_v1.yaml`: New schema for Ryan's Rocket Money exports.
    *   `rules/jordyn_wellsfargo_visa_v1.yaml`: Initial schema for Jordyn's Wells Fargo Visa statements.
    *   These schemas are based on diagnostic evidence and include detailed `header_signature`, `column_map`, `derived_columns`, and other transformation rules.

3.  **Updated Generic Fallback Schema:**
    *   `rules/generic_csv.yaml`: Converted to the new self-contained format to ensure compatibility with the updated loader and provide a consistent fallback mechanism.

4.  **Enhanced Diagnostics & Transformation Logic:**
    *   `src/balance_pipeline/cli.py`:
        *   Ensured `DEBUG` level logging for key pipeline modules when `--verbose` is active, improving diagnostic visibility.
        *   Temporarily enabled single-file processing in full ETL mode for easier iterative testing.
    *   `src/balance_pipeline/csv_consolidator.py`:
        *   Corrected `DataSourceName` population logic to prioritize values from `derived_columns` in schemas.

Next Steps:

*   Continue creating explicit, evidence-based YAML schemas for the remaining CSV formats:
    *   Jordyn's Chase statements
    *   Jordyn's Discover statements
*   Thoroughly test each new schema individually using the enhanced diagnostic process.
*   Perform system-wide integration testing with a mix of all CSV formats to ensure correct schema routing and processing.
*   Systematically remove or comment out the corresponding old entries from `rules/schema_registry.yml` as new explicit schemas are validated, completing the transition for each data source.
*   Potentially phase out `rules/schema_registry.yml` entirely once all CSV formats are covered by the new self-contained schema files.

This work establishes the core infrastructure for a more reliable and transparent data processing pipeline.
